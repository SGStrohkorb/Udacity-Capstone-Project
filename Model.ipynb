{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Models And Getting Data\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This jupyter notebook has the ability to update the Web_Get functions, get match by match data from Web_Get, aid in model selection, and has two working models for GaussianNB and XGB classifier.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updater\n",
    "updater is a function that updates the data from Web_Get. This comes in handy when something changes about the calculations in Web_Get or the data from the Blue Alliance updates. This doesn't happen for this project, because all of the matches have been played. It will take about 20ish minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def updater():\n",
    "    import ipynb.fs.defs.Web_Get as wg\n",
    "    from tqdm import tqdm\n",
    "    print(dir(wg))\n",
    "    events = wg.get_all_events(6, False)\n",
    "    \n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            #print(event)\n",
    "            new=True\n",
    "            #new=False\n",
    "            #wg.get_rankings(event, new=new)\n",
    "            #wg.get_matches(event, new=new)\n",
    "            wg.get_first_pred(event, new=new)\n",
    "            #wg.more_team_stats(event, new=new)\n",
    "            wg.predict_matches(event, new=new)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#updater()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Match Data\n",
    "get_match_data is a function that gets the match by match data for a given set of parameters. The week parameter allow for certain weeks to be used in getting the data. The only parameter allows for just one week to be chosen, else the chosen week and all previous weeks will be used. The with_rs parameter allows for all statistics involving ranking score to be dropped from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_data(week=6, with_rs=True, only=False):\n",
    "    import ipynb.fs.defs.Web_Get as wg\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "\n",
    "    events = wg.get_all_events(week, only)\n",
    "    \n",
    "    n = 0\n",
    "    print(\"Loading data\")\n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            stats, blue_wins = wg.predict_matches(event)\n",
    "            y_tmp = blue_wins\n",
    "            x_tmp = stats\n",
    "            if n == 0:\n",
    "                X = x_tmp\n",
    "                y = y_tmp\n",
    "                n += 1\n",
    "            else:\n",
    "                X = X.append(x_tmp, ignore_index=True)\n",
    "                y = np.append(y, y_tmp)\n",
    "            pbar.update(1)\n",
    "    if not with_rs:\n",
    "        X.drop(['Ranking_Score_a', 'tba_rpEarned_OAVE', 'tba_rpEarned_CPR', 'tba_rpEarned_OPR', 'tba_rpEarned_DAVE', 'tba_rpEarned_DPR'], axis=1, inplace=True)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = get_match_data(week=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "model_selection is a function that produces data to help with model selection. There is a function in the 'visuals.py' in this same directory that visualizes the data from this function. The inputs are a set of X and y data and if PCA is to be used for the model selection. If pca is true, then it is assumed the input data has already been PCA transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X, y, pca=True):\n",
    "    from sklearn.metrics import fbeta_score, accuracy_score, average_precision_score\n",
    "    from time import clock\n",
    "    beta = 0.5\n",
    "    random_state = 42\n",
    "    \n",
    "    if pca == True:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        X = MinMaxScaler([-1,1]).fit_transform(X)\n",
    "    else:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        headers = X.columns.values\n",
    "        fit_scaler = MinMaxScaler([-1,1]).fit(X[headers])\n",
    "        X[headers] = fit_scaler.transform(X[headers])\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.15,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    print(\"Training set has {} samples and {} features.\".format(X_train.shape[0], X_train.shape[1]))\n",
    "    print(\"Testing set has {} samples and {} features.\".format(X_test.shape[0], X_train.shape[1]))\n",
    "    \n",
    "    results = {}\n",
    "    test_preds = {}\n",
    "    def store_results(classifier, ypred_train, ypred_test, time):\n",
    "        evals = []\n",
    "        evals.append(accuracy_score(y_train, ypred_train))\n",
    "        evals.append(accuracy_score(y_test, ypred_test))\n",
    "        evals.append(fbeta_score(y_test, ypred_test, beta = beta))\n",
    "        evals.append(average_precision_score(y_test, ypred_test))\n",
    "        evals.append(time)\n",
    "        results[classifier] = evals\n",
    "        \n",
    "        test_preds[classifier] = ypred_test\n",
    "        \n",
    "    \n",
    "    #XGBoost\n",
    "    import xgboost as xgb\n",
    "    clf = xgb.XGBClassifier(random_state=random_state)\n",
    "    tic = clock()\n",
    "    clf.fit(X_train, y_train)\n",
    "    toc = clock()-tic\n",
    "    ypred_train = clf.predict(X_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "    store_results('XGB Classifier', ypred_train, ypred_test, toc)\n",
    "        \n",
    "    #SVC\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC(random_state=random_state, cache_size=1000)\n",
    "    tic = clock()\n",
    "    clf.fit(X_train, y_train)\n",
    "    toc = clock()-tic\n",
    "    ypred_train = clf.predict(X_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "    store_results('SVC', ypred_train, ypred_test, toc)\n",
    "    \n",
    "    #AdaBoostClassifier with DecisionTreeClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state=random_state), random_state=random_state)\n",
    "    tic = clock()\n",
    "    clf.fit(X_train, y_train)\n",
    "    toc = clock()-tic\n",
    "    ypred_train = clf.predict(X_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "    store_results('AdaBoostClassifier', ypred_train, ypred_test, toc)\n",
    "    \n",
    "    #Gaussian Naive Bayes\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    tic = clock()\n",
    "    clf.fit(X_train, y_train)\n",
    "    toc = clock()-tic\n",
    "    ypred_train = clf.predict(X_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "    store_results('GaussianNB', ypred_train, ypred_test, toc)\n",
    "    \n",
    "    #Logistic Regression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=random_state)\n",
    "    tic = clock()\n",
    "    clf.fit(X_train, y_train)\n",
    "    toc = clock()-tic\n",
    "    ypred_train = clf.predict(X_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "    store_results('Logistic Regression', ypred_train, ypred_test, toc)\n",
    "    \n",
    "    return results, test_preds, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_selection(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB Model\n",
    "This is the model for GaussianNB. This allows for easy customization of the model fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNB_model(var_smoothing, X_train, y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model\n",
    "This is the model for the XGBoost classifier. This uses a grid search algorithm to find the best preforming set of hyperparameters. For the final model, only one set of parameters are input into the model, but this robust setup is very useful for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_model(max_depth, learning_rate, n_estimators, X_train, y_train, beta, random_state, return_grid_fit=False):\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "    from sklearn.metrics import fbeta_score, accuracy_score, make_scorer, average_precision_score\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    clf = xgb.XGBClassifier(random_state=random_state)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    parameters = {'max_depth':max_depth, 'learning_rate':learning_rate, 'n_estimators':n_estimators}\n",
    "\n",
    "    cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.20)\n",
    "\n",
    "    scorer1 = make_scorer(fbeta_score, beta=beta)\n",
    "    scorer2 = make_scorer(accuracy_score)\n",
    "    scorer3 = make_scorer(average_precision_score)\n",
    "    scoring = {'scorer1': scorer1, 'scorer2': scorer2,'scorer3': scorer3}\n",
    "\n",
    "    grid_obj = GridSearchCV(clf, parameters, scoring=scoring, refit='scorer1', n_jobs=8, cv=cv, verbose=0)\n",
    "\n",
    "    grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    \n",
    "    if return_grid_fit:\n",
    "        return clf, best_clf, grid_fit\n",
    "    else:\n",
    "        return clf, best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Matches GaussianNB\n",
    "future_matches_GNB is a function that trains the GaussianNB model on the weeks prior to the input week and tests the preformance on the input week. Thus, these are the future match predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_matches_GNB(week, with_rs=True, only=False, num_pca=5):\n",
    "    from sklearn.metrics import fbeta_score, accuracy_score, average_precision_score\n",
    "    import ipynb.fs.defs.Web_Get as wg\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    beta = 0.5\n",
    "    random_state = 42\n",
    "    \n",
    "    events = wg.get_all_events(week=week-1, only=only)\n",
    "    \n",
    "    n = 0\n",
    "    print(\"Loading match data to train on\")\n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            stats, blue_wins = wg.predict_matches(event)\n",
    "            y_tmp = blue_wins\n",
    "            x_tmp = stats\n",
    "            if n == 0:\n",
    "                X = x_tmp\n",
    "                y = y_tmp\n",
    "                n += 1\n",
    "            else:\n",
    "                X = X.append(x_tmp, ignore_index=True)\n",
    "                y = np.append(y, y_tmp)\n",
    "            pbar.update(1)\n",
    "    if not with_rs:\n",
    "        X.drop(['Ranking_Score_a', 'tba_rpEarned_OAVE', 'tba_rpEarned_CPR', 'tba_rpEarned_OPR', 'tba_rpEarned_DAVE', 'tba_rpEarned_DPR'], axis=1, inplace=True)\n",
    "    #print(X.shape)\n",
    "    #print(y.shape)\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=num_pca)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    X_fit = MinMaxScaler([-1,1]).fit(X)\n",
    "    X = X_fit.transform(X)\n",
    "    \n",
    "    print(\"Training set has {} samples and {} features.\".format(X.shape[0], X.shape[1]))\n",
    "    \n",
    "    clf = GaussianNB_model([1e-9], X, y)\n",
    "    \n",
    "    events = wg.get_all_events(week=week, only=True)\n",
    "    \n",
    "    n = 0\n",
    "    print()\n",
    "    print(\"Loading future data to predict on\")\n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            stats, blue_wins = wg.predict_matches(event)\n",
    "            preds = wg.get_first_pred(event)\n",
    "            \n",
    "            y_tmp = blue_wins\n",
    "            x_tmp = stats\n",
    "            \n",
    "            pred = np.array([1 if x=='blue' else 0 for x in preds[:,0]])\n",
    "            actual = np.array([1 if x=='blue' else 0 for x in preds[:,1]])\n",
    "            \n",
    "            if n == 0:\n",
    "                X1 = x_tmp\n",
    "                y1 = y_tmp\n",
    "                pred1 = pred\n",
    "                actual1 = actual\n",
    "                n += 1\n",
    "            else:\n",
    "                X1 = X1.append(x_tmp, ignore_index=True)\n",
    "                y1 = np.append(y1, y_tmp)\n",
    "                pred1 = np.append(pred1, pred)\n",
    "                actual1 = np.append(actual1, actual)\n",
    "            pbar.update(1)\n",
    "\n",
    "    if not with_rs:\n",
    "        X1.drop(['Ranking_Score_a', 'tba_rpEarned_OAVE', 'tba_rpEarned_CPR', 'tba_rpEarned_OPR', 'tba_rpEarned_DAVE', 'tba_rpEarned_DPR'], axis=1, inplace=True)\n",
    "    X1 = pca.transform(X1)\n",
    "    X1 = X_fit.transform(X1)\n",
    "\n",
    "    predictions = clf.predict(X1)\n",
    "    \n",
    "    index = ['model_accuracy_score','model_fbeta_score','model_average_precision_score','first_accuracy_score','first_fbeta_score','first_average_precision_score','training_set_size','testing_set_size']\n",
    "\n",
    "    raw_results = [accuracy_score(y1, predictions), fbeta_score(y1, predictions, beta = beta), average_precision_score(y1, predictions), accuracy_score(actual1, pred1), fbeta_score(actual1, pred1, beta = beta), average_precision_score(actual1, pred1),X.shape[0],X1.shape[0]]\n",
    "    \n",
    "    results = pd.DataFrame(raw_results, index=index)\n",
    "    \n",
    "    print(accuracy_score(y1, predictions))\n",
    "    print(fbeta_score(y1, predictions, beta = beta))\n",
    "    print(average_precision_score(y1, predictions))\n",
    "    print()\n",
    "    print(accuracy_score(actual1, pred1))\n",
    "    print(fbeta_score(actual1, pred1, beta = beta))\n",
    "    print(average_precision_score(actual1, pred1))\n",
    "    \n",
    "    mine_real = np.equal(predictions, y1)\n",
    "    first_real = np.equal(pred1, y1)\n",
    "    \n",
    "    both_wrong = 0\n",
    "    both_right = 0\n",
    "    mine_right = 0\n",
    "    first_right = 0\n",
    "    for i, j in zip(mine_real, first_real):\n",
    "        if i == False and j == False:\n",
    "            both_wrong += 1\n",
    "        elif i == True and j == True:\n",
    "            both_right += 1\n",
    "        elif i == True and j == False:\n",
    "            mine_right += 1\n",
    "        elif i == False and j == True:\n",
    "            first_right += 1\n",
    "            \n",
    "    return both_right, both_wrong, mine_right, first_right, X1, predictions, pred1, y1, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#future_matches_GNB(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Matches XGB\n",
    "future_matches_XGB is a function that trains the GaussianNB model on the weeks prior to the input week and tests the preformance on the input week. Thus, these are the future match predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_matches_XGB(week, with_rs=True, only=False):\n",
    "    from sklearn.metrics import fbeta_score, accuracy_score, average_precision_score\n",
    "    import ipynb.fs.defs.Web_Get as wg\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    beta = 0.5\n",
    "    random_state = 42\n",
    "    \n",
    "    events = wg.get_all_events(week=week-1, only=only)\n",
    "    \n",
    "    n = 0\n",
    "    print(\"Loading match data to train on\")\n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            stats, blue_wins = wg.predict_matches(event)\n",
    "            y_tmp = blue_wins\n",
    "            x_tmp = stats\n",
    "            if n == 0:\n",
    "                X = x_tmp\n",
    "                y = y_tmp\n",
    "                n += 1\n",
    "            else:\n",
    "                X = X.append(x_tmp, ignore_index=True)\n",
    "                y = np.append(y, y_tmp)\n",
    "            pbar.update(1)\n",
    "    if not with_rs:\n",
    "        X.drop(['Ranking_Score_a', 'tba_rpEarned_OAVE', 'tba_rpEarned_CPR', 'tba_rpEarned_OPR', 'tba_rpEarned_DAVE', 'tba_rpEarned_DPR'], axis=1, inplace=True)\n",
    "    #print(X.shape)\n",
    "    #print(y.shape)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    headers = X.columns.values\n",
    "    fit_scaler = MinMaxScaler([-1,1]).fit(X[headers])\n",
    "    X[headers] = fit_scaler.transform(X[headers])\n",
    "    \n",
    "    print(\"Training set has {} samples and {} features.\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "    clf, best_clf = XGBoost_model([3], [0.1], [100], X, y, beta, random_state)\n",
    "    \n",
    "    events = wg.get_all_events(week=week, only=True)\n",
    "    \n",
    "    n = 0\n",
    "    print()\n",
    "    print(\"Loading future data to predict on\")\n",
    "    with tqdm(total=len(events)) as pbar:\n",
    "        for event in events:\n",
    "            stats, blue_wins = wg.predict_matches(event)\n",
    "            preds = wg.get_first_pred(event)\n",
    "            \n",
    "            y_tmp = blue_wins\n",
    "            x_tmp = stats\n",
    "            \n",
    "            pred = np.array([1 if x=='blue' else 0 for x in preds[:,0]])\n",
    "            actual = np.array([1 if x=='blue' else 0 for x in preds[:,1]])\n",
    "            \n",
    "            if n == 0:\n",
    "                X1 = x_tmp\n",
    "                y1 = y_tmp\n",
    "                pred1 = pred\n",
    "                actual1 = actual\n",
    "                n += 1\n",
    "            else:\n",
    "                X1 = X1.append(x_tmp, ignore_index=True)\n",
    "                y1 = np.append(y1, y_tmp)\n",
    "                pred1 = np.append(pred1, pred)\n",
    "                actual1 = np.append(actual1, actual)\n",
    "            pbar.update(1)\n",
    "\n",
    "    if not with_rs:\n",
    "        X1.drop(['Ranking_Score_a', 'tba_rpEarned_OAVE', 'tba_rpEarned_CPR', 'tba_rpEarned_OPR', 'tba_rpEarned_DAVE', 'tba_rpEarned_DPR'], axis=1, inplace=True)\n",
    "\n",
    "    headers = X1.columns.values\n",
    "    X1[headers] = fit_scaler.transform(X1[headers])\n",
    "\n",
    "    predictions = best_clf.predict(X1)\n",
    "    \n",
    "    index = ['model_accuracy_score','model_fbeta_score','model_average_precision_score','first_accuracy_score','first_fbeta_score','first_average_precision_score','training_set_size','testing_set_size']\n",
    "\n",
    "    raw_results = [accuracy_score(y1, predictions), fbeta_score(y1, predictions, beta = beta), average_precision_score(y1, predictions), accuracy_score(actual1, pred1), fbeta_score(actual1, pred1, beta = beta), average_precision_score(actual1, pred1),X.shape[0],X1.shape[0]]\n",
    "    \n",
    "    results = pd.DataFrame(raw_results, index=index)\n",
    "\n",
    "    print(accuracy_score(y1, predictions))\n",
    "    print(fbeta_score(y1, predictions, beta = beta))\n",
    "    print(average_precision_score(y1, predictions))\n",
    "    print()\n",
    "    print(accuracy_score(actual1, pred1))\n",
    "    print(fbeta_score(actual1, pred1, beta = beta))\n",
    "    print(average_precision_score(actual1, pred1))\n",
    "    \n",
    "    mine_real = np.equal(predictions, y1)\n",
    "    first_real = np.equal(pred1, y1)\n",
    "    \n",
    "    both_wrong = 0\n",
    "    both_right = 0\n",
    "    mine_right = 0\n",
    "    first_right = 0\n",
    "    for i, j in zip(mine_real, first_real):\n",
    "        if i == False and j == False:\n",
    "            both_wrong += 1\n",
    "        elif i == True and j == True:\n",
    "            both_right += 1\n",
    "        elif i == True and j == False:\n",
    "            mine_right += 1\n",
    "        elif i == False and j == True:\n",
    "            first_right += 1\n",
    "            \n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X1)\n",
    "    X1_pca = pca.transform(X1)\n",
    "            \n",
    "    return both_right, both_wrong, mine_right, first_right, X1_pca, predictions, pred1, y1, best_clf, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#future_matches_XGB(2, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
